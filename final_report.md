# Porject 5: Identify Fraud from Enron Email
---
**Udacity Data Analyst Nonedegree**

By Nan-Tsou Liu @ 2016-07-11

## Introduction
<p>
<a href=https://en.wikipedia.org/wiki/Enron>Enron Corporation</a> was an American energy, commodities, and services company based in Houston, Texas. Before its bankruptcy in 2001, there were about 20,000 employees and was one of the world's major electricity, natural gas, communications and pulp and paper companies.
</p>
<p>
<a href=https://en.wikipedia.org/wiki/Enron_Corpus>The Enron Corpus</a> is a large database of over 600,000 emails generated by 158 employees of the Enron Corporation and acquired by the Federal Energy Regulatory Commission during its investigation after the company's collapse. 
</p>

## Short Question

>Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those? <br/>
[relevant rubric items: “data exploration”, “outlier investigation”]


###Goal of the Project:

<p>
In this project, the prediction model is built by using the python module called scikit-learn to identify the person of interest (POI). The dataset contains 146 records with 1 labele (POI), 14 financial features and 6 email feature. In the data of labels, there are 18 records have been marked as POI. The following skills are applied to carried out the project:
<ul>
<li>feature selection and scaling</li>
<li>algorithm selection and tuning</li>
<li>validation and classic mistakes</li>
<li>evaluation metrics and interpretation of algorithm's performance</li>
</ul>
</p>

###Outliers
<p>
By observing the data and pdf file, enron61702insiderpay.pdf, the obvious outliers are <strong>TOTAL</strong> and <strong>THE TRAVEL AGENCY IN THE PARK</strong> which are simply removed by <code>pop()</code> method of dictionary in Python. Besides, the record of <strong>LOCKHART EUGENE E</strong> is empty, thus it was also removed.
</p>
<p>
I found out that the records of <strong>BELFER ROBERT</strong> and <strong>BHATNAGAR SANJAY</strong> are not consistent with the data in pdf file by accident. Therefore, I fixed them before removing outliers and replacing NaN with 0. I did not check every records so that I am not sure whether there are others not consistent or not.
</p>

>What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values.

<p>
At the beginning, I used all the original features except <strong>mail_address</strong> in the dataset. Besides, I added <strong>financial relatived features</strong> like the ratios of each payment feature or stock feature to the total amount of financial and <strong>message related feature</strong>.
</p> 
<p>
The reasons I added these features are that first, I assumed that POI has somehow great relationship with the financial status. I calculated the ratio of each financial features to <strong>total_financial</strong> (summation of <strong>total_payments</strong> and <strong>total_stock_value</strong>) because I thought that the person of POI might had large percentage of restricted_stock or salary of the total financial status. It also means that <strong>the composition of the financial status</strong> might be the good features for model training.
</p>
<p>
On the other hand, the messages of each person should be a strong feature to identify POI. Therefore, I culaculated the ratio of <strong>poi_relative_message</strong> (summation of <strong>from_poi_to_this_person</strong>, <strong>from_this_person_to_poi</strong> and shared <strong>receipt_with_poi</strong>) to <strong>total_messages</strong> (summation of <strong>from_message</strong> and <strong>to_message</strong>).
</p>

###Added Features

<table>
<thead>
<tr>
<td><strong>Feature</strong></td>
<td><strong>Description</strong></td>
</tr>
</thead>
<tbody>
<tr>
<td><strong>total_financial</strong></td>
<td>total_payments + total_stock_value</td>
</tr>
<tr>
<td><strong>{each feature}_ratio</strong></td>
<td>{each feature} / total_financial</td>
</tr>
<tr>
<td><strong>poi_ratio_messages</strong></td>
<td>poi_related_messages / total_messages</td>
</tr>
<tr>
<td><strong>poi_related_messages</strong></td>
<td>from_poi_to_this_person + from_this_person_to_poi + shared_receipt_with_poi (NOT USED)</td>
</tr>
<tr>
<td><strong>total_messages</strong></td>
<td>from_messages + to_messages (NOT USED)</td>
</tr>
</tbody>
</table>

### Engineering Data

<p>
The end-up using features were selected during GridSearchCV pipeline search with following steps:
<ol>
<li>scale all features to be between 0 and 1 with MinMaxScaler</li>
<li>dimension reduction with SelectKBest and Principal Components Analysis</li>
<li>tune parameters of each models</li>
</ol>
Features scaling was carried out since PCA and various models such as Logistic Regression perform optimally with scaled features. Feature scaling is also necessary since they were on very different scales, ranging from hundreds of e-mails to millions of dollars.
</p>
<p>
<strong>SelectKBest</strong> and <strong>Principal Components Analysis (PCA)</strong> dimension reduction were run during each of the cross-validation loops during the grid search. The K-best features were selected using <strong>Anova F-value classification</strong> scoring function. The K-best features were then used in reducing dimension with PCA. Finally,the N principal components were fed into a classification algorithm.
</p>

###K-Best Feature (Top 17)


>What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?

<p>
The ended up using algorithm was determined by the results of <strong>GridSearchCV</strong> respected <strong>recall score</strong> and <strong>precision score</strong> with k-best feature selection, PCA reduction and parameters tuning. <strong>Support Vector Classifier (SVC)</strong> showed the best results, and therefore it was the ended up using algorithm in this project. Besides SVC, I also tried <strong>Logistic Regression (LogReg)</strong>, <strong>Linear Support Vector Machine (LSVC)</strong>, <strong>Decision Tree (DTree)</strong> and <strong>K-Means Classifier (KMeans)</strong>.
</p>
<p>
Actually, LogReg and LSVC also showed the competive results compared with that of SVC. And the results of both algorithms were similar. According to the obversation by manual parameter tuning, recall score and precision score were changed against with each other. And the parameter <strong>C</strong> affected the results mostly. And I found out an interesting phenomenon that the value ended with 5 like <code>[0.05, 0.5, 0.15]</code> could keep recall score at good value and promote precision score well. And, precision score kelp <strong>around 0.3</strong>.
</p>

<p>
On the other hand, the precision score of KMeans kept at the value <strong>about 0.15</strong>, which is far from the requirement of the assignment. No matter how I tuned the parameters, although precision score was about <strong>0.5</strong>. In my opinion, KMeans might not suitable for this prediction after I added the new features.
</p>

#TODO table of results

>What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  How did you tune the parameters of your particular algorithm? (Some algorithms do not have parameters that you need to tune -- if this is the case for the one you picked, identify and briefly explain how you would have done it for the model that was not your final choice or a different model that does utilize parameter tuning, e.g. a decision tree classifier). 

<p>
Tuning the parameters of an algorithm is a process to promote the performance of the model. Depended on the structure and nature of the dataset, tuning the parameters would cost lots of time when doing model training. In this project, <strong>Grid Search</strong> was used to tune the parameters of the algorithms with 1000 randomized stratified cross-validation stratified splits. The parameters of each algorithms with highest average score were choosen for the models.
</p>

### Final Results of each Algorithm
<table>
<thead>
<tr>
<td><strong>Parameter</strong></td>
<td><strong>Logistic Regression</strong></td>
<td><strong>Linear Support Vector Classifier</strong></td>
<td><strong>Support Vector Classifier</strong></td>
</tr>
</thead>
<tbody>
<tr>
<td><strong>C</strong></td>
<td>0.5</td>
<td>0.15</td>
<td>0.01</td>
</tr>
<tr>
<td><strong>class_wight</strong></td>
<td>auto</td>
<td>auto</td>
<td>auto</td>
</tr>
<tr>
<td><strong>tol</strong></td>
<td>1e-64</td>
<td>1e-32</td>
<td>1e-8</td>
</tr>
<tr>
<td><strong>n_components of PCA</strong></td>
<td>0.5</td>
<td>0.5</td>
<td>0.5</td>
</tr>
<tr>
<td><strong>whiten of PCA</strong></td>
<td>True</td>
<td>False</td>
<td>False</td>
</tr>
<tr>
<td><strong>selection of SelectKBest</strong></td>
<td>17</td>
<td>15</td>
<td>15</td>
</tr>
<tr>
<td><strong>gamma</strong></td>
<td>-</td>
<td>-</td>
<td>0.0</td>
</tr>
<tr>
<td><strong>kernel</strong></td>
<td>-</td>
<td>linier</td>
<td>rbf</td>
</tr>
</tbody>
</table>

>What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis?

<p>
Validation is the process to ensure that the results produced built model with other unknown data is reliable. In my case, A classic mistake is over-fitting, which makes the model too particular so that the results produced with the other data are poor and unreliable. One of major purpose of validation is to avoid over-fitting.
</p>

### Cross-Validation
<p>
Cross-Validation was applied on the validation of model. It is a process that randomly split the data into training and testing dataset. And then it trains the model with the training data and validates with the testing data.
In this project, the whole dateset was splitted with 1000 randomized <strong>stratified cross-validation splits</strong>. And then the parameters with the best performance over 1000 splits were selected.
</p>

### Parameter Tuning
<p>
As I mentioned above, <strong>GridSearchCV</strong> with over 1000 stratified shuffled cross-validation 90%-training/ 10%-testing splits was used to tune the parameters in this project. Besides, K-best selection and PCA reduction processes were embraced into the parameter tuning loop. Compared with outside selection, the selection in the loop might promote the consistence of parameter tuning and give a less biased estimate of performance on any new unseen data that this model might be used for.
</p>
<p>
As the results of selected the final model, 15 features were selected. PCA reduction gave 2 principal components. These parameters were used in the final <strong>Support Vector Machine</strong> classification model.
</p>
<p>
One should be notified is that these features might change slightly each time since the k-best selection was carried out inside of the pipeline. Below are the final 15 features chosen when the entire dataset was fit to the final chosen model pipeline:
</p>

<table>
<thead>
<tr>
<td><strong>feature</strong></td>
<td><strong>score↑</strong></td>
</tr>
</thead>
<tbody>
<tr>
<td><strong>total_stock_value</strong></td>
<td>22.5105490902</td>
</tr>
<tr>
<td><strong>exercised_stock_options</strong></td>
<td>22.3489754073</td>
</tr>
<tr>
<td><strong>bonus</strong></td>
<td>20.7922520472</td>
</tr>
<tr>
<td><strong>salary</strong></td>
<td>18.2896840434</td>
</tr>
<tr>
<td><strong>deferred_income</strong></td>
<td>11.4248914854</td>
</tr>
<tr>
<td><strong>poi_ratio_messages</strong></td>
<td>10.0194150056</td>
</tr>
<tr>
<td><strong>long_term_incentive</strong></td>
<td>9.92218601319</td>
</tr>
<tr>
<td><strong>total_payments</strong></td>
<td>9.28387361843</td>
</tr>
<tr>
<td><strong>restricted_stock</strong></td>
<td>8.83185274222</td>
</tr>
<tr>
<td><strong>shared_receipt_with_poi</strong></td>
<td>8.58942073168</td>
</tr>
<tr>
<td><strong>loan_advances</strong></td>
<td>7.18405565829</td>
</tr>
<tr>
<td><strong>bonus_ratio</strong></td>
<td>6.58326879249</td>
</tr>
<tr>
<td><strong>expenses</strong></td>
<td>5.41890018941</td>
</tr>
<tr>
<td><strong>from_poi_to_this_person</strong></td>
<td>5.24344971337</td>
</tr>
<tr>
<td><strong>loan_advances_ratio</strong></td>
<td>4.56000450411</td>
</tr>
</tbody>
</table>

>Give at least 2 evaluation metrics and your average performance for each of them.  Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance.

### Performance Analysis
<p>
<strong>Recall</strong> and <strong>Precision</strong> were used as the primary evaluattion metrics. The definition are shown below:
</p>
<p>
<code>recall = True_Positive / (True_Positive + False_Negative)</code>
</p>
<p>
With high recall score, 
</p>
<p>
<code>precision = True_Positive / (True_Positive + False_Positive)</code>
</p>
<p>
Have an overall observation, recall scores are greater than precision scores of all the results in project. 
</p>

## Conclusion

SVC
Cross-validated recall score: 0.716

F1 Avg:  0.440757620158
Precision Avg:  0.33174491342
Recall Avg:  0.7335

## Reference