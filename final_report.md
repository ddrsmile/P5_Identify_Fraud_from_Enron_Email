# Porject 5: Identify Fraud from Enron Email
---
**Udacity Data Analyst Nonedegree**

By Nan-Tsou Liu @ 2016-07-11

## Introduction
<p>
<a href=https://en.wikipedia.org/wiki/Enron>Enron Corporation</a> was an American energy, commodities, and services company based in Houston, Texas. Before its bankruptcy in 2001, there were about 20,000 employees and was one of the world's major electricity, natural gas, communications and pulp and paper companies.
</p>
<p>
<a href=https://en.wikipedia.org/wiki/Enron_Corpus>The Enron Corpus</a> is a large database of over 600,000 emails generated by 158 employees of the Enron Corporation and acquired by the Federal Energy Regulatory Commission during its investigation after the company's collapse. 
</p>

## Short Question

>Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those? <br/>
[relevant rubric items: “data exploration”, “outlier investigation”]


###Goal of the Project:

<p>
In this project, the prediction model is built by using the python module called scikit-learn to identify the person of interest (POI). The dataset contains 146 records with 1 labele (POI), 14 financial features and 6 email feature. In the data of labels, there are 18 records have been marked as POI. The following skills are applied to carried out the project:
<ul>
<li>feature selection and scaling</li>
<li>algorithm selection and tuning</li>
<li>validation and classic mistakes</li>
<li>evaluation metrics and interpretation of algorithm's performance</li>
</ul>
</p>

###Outliers
<p>
By observing the data and pdf file, enron61702insiderpay.pdf, the obvious outliers are <strong>TOTAL</strong> and <strong>THE TRAVEL AGENCY IN THE PARK</strong> which are simply removed by <code>pop()</code> method of dictionary in Python. Besides, the record of <strong>LOCKHART EUGENE E</strong> is empty, thus it was also removed.
</p>
<p>
I found out that the records of <strong>BELFER ROBERT</strong> and <strong>BHATNAGAR SANJAY</strong> are not consistent with the data in pdf file by accident. Therefore, I fixed them before removing outliers and replacing NaN with 0. I did not check every records so that I am not sure whether there are others not consistent or not.
</p>

>What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values.

<p>
At the beginning, I used all the original features except <strong>mail_address</strong> in the dataset. Besides, I added <strong>financial relatived features</strong> like the ratios of each payment feature or stock feature to the total amount of financial and <strong>message related feature</strong>.
</p> 
<p>
The reasons I added these features are that first, I assumed that POI has somehow great relationship with the financial status. I calculated the ratio of each financial features to <strong>total_financial</strong> (summation of <strong>total_payments</strong> and <strong>total_stock_value</strong>) because I thought that the person of POI might had large percentage of restricted_stock or salary of the total financial status. It also means that <strong>the composition of the financial status</strong> might be the good features for model training.
</p>
<p>
On the other hand, the messages of each person should be a strong feature to identify POI. Therefore, I culaculated the ratio of <strong>poi_relative_message</strong> (summation of <strong>from_poi_to_this_person</strong>, <strong>from_this_person_to_poi</strong> and shared <strong>receipt_with_poi</strong>) to <strong>total_messages</strong> (summation of <strong>from_message</strong> and <strong>to_message</strong>).
</p>

###Added Features

<table>
<thead>
<tr>
<td><strong>Feature</strong></td>
<td><strong>Description</strong></td>
</tr>
</thead>
<tbody>
<tr>
<td><strong>total_financial</strong></td>
<td>total_payments + total_stock_value</td>
</tr>
<tr>
<td><strong>{each feature}_ratio</strong></td>
<td>{each feature} / total_financial</td>
</tr>
<tr>
<td><strong>poi_ratio_messages</strong></td>
<td>poi_related_messages / total_messages</td>
</tr>
<tr>
<td><strong>poi_related_messages</strong></td>
<td>from_poi_to_this_person + from_this_person_to_poi + shared_receipt_with_poi (NOT USED)</td>
</tr>
<tr>
<td><strong>total_messages</strong></td>
<td>from_messages + to_messages (NOT USED)</td>
</tr>
</tbody>
</table>

### Engineering Data

<p>
The end-up using features were selected during GridSearchCV pipeline search with following steps:
<ol>
<li>scale all features to be between 0 and 1 with MinMaxScaler</li>
<li>dimension reduction with SelectKBest and Principal Components Analysis</li>
<li>tune parameters of each models</li>
</ol>
Features scaling was carried out since PCA and various models such as Logistic Regression perform optimally with scaled features. Feature scaling is also necessary since they were on very different scales, ranging from hundreds of e-mails to millions of dollars.
</p>
<p>
<strong>SelectKBest</strong> and <strong>Principal Components Analysis (PCA)</strong> dimension reduction were run during each of the cross-validation loops during the grid search. The K-best features were selected using <strong>Anova F-value classification</strong> scoring function. The K-best features were then used in reducing dimension with PCA. Finally,the N principal components were fed into a classification algorithm.
</p>


>What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?

<p>
<strong>Support Vector Classifier (SVC)</strong> is the algorithm I ended up using because the results produced by SVC was the best with the features I added. I also tried the algorithms such as Linear Regression Classifier, Logistic Regression Classifier and Linear Support Vector Classifier. 
</p>

abc

>What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  How did you tune the parameters of your particular algorithm? (Some algorithms do not have parameters that you need to tune -- if this is the case for the one you picked, identify and briefly explain how you would have done it for the model that was not your final choice or a different model that does utilize parameter tuning, e.g. a decision tree classifier). 

<p>
Tuning the parameters of an algorithm is a process to promote the performance of the model. Depended on the structure and nature of the dataset, tuning the parameters would cost lots of time when doing model training. In this project, <strong>Grid Search</strong> was used to tune the parameters of the algorithms with 1000 randomized stratified cross-validation stratified splits. The parameters of each algorithms with highest average score were choosen for the models.
</p>

### Final Results of each Algorithm
<table>
<thead>
<tr>
<td><strong>Parameter</strong></td>
<td><strong>Logistic Regression</strong></td>
<td><strong>Linear Support Vector Classifier</strong></td>
<td><strong>Support Vector Classifier</strong></td>
</tr>
</thead>
<tbody>
<tr>
<td><strong>C</strong></td>
<td>0.1</td>
<td>?</td>
<td>0.01</td>
</tr>
<tr>
<td><strong>class_wight</strong></td>
<td>auto</td>
<td>auto</td>
<td>auto</td>
</tr>
<tr>
<td><strong>tol</strong></td>
<td>1e-32</td>
<td>?</td>
<td>1e-8</td>
</tr>
<tr>
<td><strong>n_components of PCA</strong></td>
<td>0.5</td>
<td>0.5</td>
<td>0.5</td>
</tr>
<tr>
<td><strong>whiten of PCA</strong></td>
<td>True</td>
<td>False</td>
<td>False</td>
</tr>
<tr>
<td><strong>selection of SelectKBest</strong></td>
<td>17</td>
<td>?</td>
<td>15</td>
</tr>
<tr>
<td><strong>gamma</strong></td>
<td>-</td>
<td>?</td>
<td>0.0</td>
</tr>
<tr>
<td><strong>kernel</strong></td>
<td>-</td>
<td>linier</td>
<td>rbf</td>
</tr>
</tbody>
</table>

>What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis?

<p>
Validation is the process to ensure that the results produced built model with other unknown data is reliable. In my case, A classic mistake is over-fitting, which makes the model too particular so that the results produced with the other data are poor and unreliable. One of major purpose of validation is to avoid over-fitting.
</p>

abc

>Give at least 2 evaluation metrics and your average performance for each of them.  Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance.

abc

## Conclusion
